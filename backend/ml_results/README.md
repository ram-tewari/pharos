# ML Results Directory

This directory stores all machine learning training, evaluation, and benchmark results for Pharos.

## Purpose

Centralized storage for:
- Model training metrics and logs
- Retrieval system audits
- Benchmark comparisons
- Performance evaluations
- Experiment tracking

## Directory Structure

```
ml_results/
├── README.md                          # This file
├── retrieval_audits/                  # Search retrieval evaluation results
│   ├── audit_YYYYMMDD_HHMMSS.json    # Timestamped audit results
│   └── ...
├── classification_training/           # Taxonomy classifier training results
│   ├── training_YYYYMMDD_HHMMSS.json # Training metrics
│   └── ...
├── recommendation_training/           # NCF model training results
│   ├── training_YYYYMMDD_HHMMSS.json # Training metrics
│   └── ...
└── benchmarks/                        # Performance benchmarks
    ├── benchmark_YYYYMMDD_HHMMSS.json # Benchmark results
    └── ...
```

## Subdirectories

### retrieval_audits/

**Purpose**: Evaluation results for search retrieval systems

**Contents**:
- FTS5 baseline performance
- Vector search performance
- Hybrid search performance
- MRR (Mean Reciprocal Rank) metrics
- Latency measurements
- Per-query results

**Generated by**:
- `backend/scripts/audit_standalone.py`
- `backend/scripts/audit_retrieval.py`

**Schema**:
```json
{
  "timestamp": "20260127_162345",
  "sample_size": 20,
  "warmup_time_seconds": 7.15,
  "metrics": {
    "fts5": {
      "mrr": 1.0,
      "avg_latency_ms": 2.2
    },
    "vector": {
      "mrr": 0.9583,
      "avg_latency_ms": 85.2
    },
    "hybrid": {
      "mrr": 1.0,
      "avg_latency_ms": 90.4
    }
  },
  "per_query_results": [...]
}
```

### classification_training/

**Purpose**: Training results for taxonomy classification models

**Contents**:
- Model accuracy metrics
- Precision, recall, F1 scores
- Confusion matrices
- Training/validation loss curves
- Hyperparameter configurations

**Generated by**:
- `backend/scripts/train_classification.py`
- `backend/scripts/training/train_taxonomy.py`

**Schema**:
```json
{
  "timestamp": "20260127_162345",
  "model_type": "random_forest",
  "hyperparameters": {...},
  "metrics": {
    "accuracy": 0.87,
    "precision": 0.85,
    "recall": 0.89,
    "f1_score": 0.87
  },
  "training_time_seconds": 45.2,
  "model_path": "models/taxonomy_classifier_20260127.pkl"
}
```

### recommendation_training/

**Purpose**: Training results for Neural Collaborative Filtering (NCF) models

**Contents**:
- Hit Rate @ K metrics
- NDCG @ K metrics
- Training/validation loss
- Embedding dimensions
- Model architecture details

**Generated by**:
- `backend/scripts/train_ncf.py`
- `backend/scripts/train_ncf_model.py`

**Schema**:
```json
{
  "timestamp": "20260127_162345",
  "model_type": "ncf",
  "architecture": {
    "embedding_dim": 64,
    "hidden_layers": [128, 64, 32]
  },
  "metrics": {
    "hit_rate_at_10": 0.72,
    "ndcg_at_10": 0.68
  },
  "training_epochs": 50,
  "training_time_seconds": 320.5,
  "model_path": "models/ncf_benchmark_v1.pt"
}
```

### benchmarks/

**Purpose**: System-wide performance benchmarks

**Contents**:
- End-to-end workflow timings
- API endpoint latency
- Database query performance
- Memory usage profiles
- Throughput measurements

**Generated by**:
- `backend/scripts/evaluation/benchmark_rag_models.py`
- `backend/tests/performance/test_phase19_benchmarks.py`

**Schema**:
```json
{
  "timestamp": "20260127_162345",
  "benchmark_type": "rag_evaluation",
  "system_info": {
    "cpu": "Intel i7-9700K",
    "ram_gb": 16,
    "gpu": "NVIDIA RTX 3060"
  },
  "results": {
    "avg_query_latency_ms": 145.3,
    "throughput_qps": 6.8,
    "memory_usage_mb": 2048
  }
}
```

## Usage

### Saving Results

All scripts should save results using timestamped filenames:

```python
from datetime import datetime
from pathlib import Path
import json

# Get results directory
results_dir = Path(__file__).parent.parent / "ml_results" / "retrieval_audits"
results_dir.mkdir(parents=True, exist_ok=True)

# Create timestamped filename
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
results_file = results_dir / f"audit_{timestamp}.json"

# Save results
with open(results_file, 'w') as f:
    json.dump(results_data, f, indent=2)
```

### Loading Results

```python
import json
from pathlib import Path

# Load latest result
results_dir = Path("backend/ml_results/retrieval_audits")
latest_file = sorted(results_dir.glob("audit_*.json"))[-1]

with open(latest_file, 'r') as f:
    results = json.load(f)
```

### Comparing Results

```python
import json
from pathlib import Path

# Load all results
results_dir = Path("backend/ml_results/retrieval_audits")
all_results = []

for file in sorted(results_dir.glob("audit_*.json")):
    with open(file, 'r') as f:
        all_results.append(json.load(f))

# Compare MRR over time
for result in all_results:
    print(f"{result['timestamp']}: MRR={result['metrics']['hybrid']['mrr']:.4f}")
```

## Best Practices

1. **Always use timestamps** - Ensures unique filenames and chronological ordering
2. **Include metadata** - System info, hyperparameters, dataset details
3. **Save raw results** - Don't just save aggregated metrics
4. **Document schema** - Update this README when adding new result types
5. **Version control** - Commit result files for reproducibility
6. **Clean old results** - Archive or delete outdated results periodically

## File Naming Convention

Format: `{type}_{YYYYMMDD}_{HHMMSS}.json`

Examples:
- `audit_20260127_162345.json` - Retrieval audit
- `training_20260127_162345.json` - Model training
- `benchmark_20260127_162345.json` - Performance benchmark

## Integration with Training Scripts

### Retrieval Audits

```bash
# Run audit (automatically saves to ml_results/retrieval_audits/)
python backend/scripts/audit_standalone.py
```

### Classification Training

```bash
# Train classifier (saves to ml_results/classification_training/)
python backend/scripts/train_classification.py
```

### Recommendation Training

```bash
# Train NCF model (saves to ml_results/recommendation_training/)
python backend/scripts/train_ncf_model.py
```

### Benchmarks

```bash
# Run benchmarks (saves to ml_results/benchmarks/)
python backend/scripts/evaluation/benchmark_rag_models.py
```

## Visualization

Future enhancements:
- Jupyter notebooks for result visualization
- Plotly/Matplotlib charts for metric trends
- Comparison dashboards
- Automated report generation

## Related Documentation

- [Testing Guide](../docs/guides/testing.md) - Testing strategies
- [ML Benchmarks History](../docs/ML_BENCHMARKS_HISTORY.json) - Historical benchmarks
- [Retrieval Audit Success](../scripts/RETRIEVAL_AUDIT_SUCCESS.md) - Audit system docs

## Maintenance

- **Retention**: Keep results for at least 6 months
- **Archival**: Move old results to `ml_results/archive/` after 1 year
- **Cleanup**: Run cleanup script quarterly to remove duplicates
- **Backup**: Results are version controlled, but consider external backup for large files

## Questions?

For questions about ML results or adding new result types, see:
- Backend documentation: `backend/docs/`
- Training scripts: `backend/scripts/training/`
- Evaluation scripts: `backend/scripts/evaluation/`
