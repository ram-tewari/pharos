"""
Comprehensive performance tests for Phase 19 - Hybrid Edge-Cloud Orchestration.

This test suite combines benchmarks and stress tests to verify:
- API dispatch latency (< 100ms)
- Embedding generation time (< 5 minutes for 100 files)
- Large repository handling (10,000 files)

Validates: Requirements 10.1, 10.2, 10.5
"""

import pytest
import time
import torch
import os
from unittest.mock import Mock, patch
from fastapi.testclient import TestClient
from upstash_redis import Redis

# Import services
from app.modules.graph.neural_service import NeuralGraphService
from app.utils.repo_parser import DependencyGraph


# ============================================================================
# Fixtures
# ============================================================================


@pytest.fixture
def mock_redis():
    """Mock Redis client for testing."""
    redis = Mock(spec=Redis)
    redis.rpush = Mock(return_value=1)
    redis.llen = Mock(return_value=0)
    redis.expire = Mock(return_value=True)
    redis.get = Mock(return_value="Idle")
    redis.set = Mock(return_value=True)
    redis.lpush = Mock(return_value=True)
    redis.ltrim = Mock(return_value=True)
    return redis


@pytest.fixture
def cloud_api_client(mock_redis):
    """FastAPI test client in CLOUD mode."""
    os.environ["MODE"] = "CLOUD"
    os.environ["PHAROS_ADMIN_TOKEN"] = "test-token-123"
    
    # Mock Redis dependency
    with patch("app.routers.ingestion.get_redis_client", return_value=mock_redis):
        from app.main import app
        client = TestClient(app)
        yield client


@pytest.fixture
def neural_service():
    """Neural graph service for testing."""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    return NeuralGraphService(device=device)


# ============================================================================
# Performance Test 1: API Dispatch Latency
# ============================================================================


@pytest.mark.performance
class TestAPIDispatchLatency:
    """Test API dispatch latency meets < 100ms requirement."""
    
    def test_api_dispatch_latency_p95(self, cloud_api_client, mock_redis):
        """
        Performance Test: API dispatch latency should be < 100ms (P95).
        
        Validates: Requirements 10.1
        """
        print("\n=== API Dispatch Latency Test ===")
        
        # Warm up
        for _ in range(5):
            cloud_api_client.post(
                "/api/v1/ingestion/ingest/github.com/test/warmup",
                headers={"Authorization": "Bearer test-token-123"}
            )
        
        # Measure latency over 50 requests
        latencies = []
        
        for i in range(50):
            start_time = time.perf_counter()
            
            response = cloud_api_client.post(
                f"/api/v1/ingestion/ingest/github.com/test/repo{i}",
                headers={"Authorization": "Bearer test-token-123"}
            )
            
            end_time = time.perf_counter()
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)
            
            assert response.status_code == 200
        
        # Calculate P95
        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
        avg_latency = sum(latencies) / len(latencies)
        
        print(f"Requests: {len(latencies)}")
        print(f"Average: {avg_latency:.2f}ms")
        print(f"P95: {p95_latency:.2f}ms")
        print(f"Target: < 100ms")
        
        # Verify requirement
        assert p95_latency < 100.0, (
            f"API dispatch latency P95 is {p95_latency:.2f}ms, "
            f"requirement is < 100ms"
        )
    
    def test_api_dispatch_consistency(self, cloud_api_client, mock_redis):
        """
        Performance Test: API latency should be consistent across requests.
        
        Validates: Requirements 10.1
        """
        print("\n=== API Dispatch Consistency Test ===")
        
        # Measure latency over 30 requests
        latencies = []
        
        for i in range(30):
            start_time = time.perf_counter()
            
            response = cloud_api_client.post(
                f"/api/v1/ingestion/ingest/github.com/consistency/repo{i}",
                headers={"Authorization": "Bearer test-token-123"}
            )
            
            end_time = time.perf_counter()
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)
            
            assert response.status_code == 200
        
        # Calculate variance
        avg_latency = sum(latencies) / len(latencies)
        variance = sum((l - avg_latency) ** 2 for l in latencies) / len(latencies)
        std_dev = variance ** 0.5
        
        print(f"Average: {avg_latency:.2f}ms")
        print(f"Std Dev: {std_dev:.2f}ms")
        print(f"Coefficient of Variation: {(std_dev / avg_latency) * 100:.1f}%")
        
        # Verify consistency (CV should be < 50%)
        cv = (std_dev / avg_latency) * 100
        assert cv < 50.0, (
            f"API latency is inconsistent: CV = {cv:.1f}%, should be < 50%"
        )


# ============================================================================
# Performance Test 2: Embedding Generation Time
# ============================================================================


@pytest.mark.performance
@pytest.mark.slow
class TestEmbeddingGenerationTime:
    """Test embedding generation time meets < 5 minutes for 100 files requirement."""
    
    def test_embedding_generation_100_files(self, neural_service):
        """
        Performance Test: Embedding generation should be < 5 minutes for 100 files.
        
        Validates: Requirements 10.2
        """
        print("\n=== Embedding Generation Time Test ===")
        
        # Create dependency graph with 100 nodes
        num_nodes = 100
        edges = []
        
        # Create chain dependencies
        for i in range(num_nodes - 1):
            edges.append([i, i + 1])
        
        # Add some cross-links
        for i in range(0, num_nodes, 10):
            if i + 5 < num_nodes:
                edges.append([i, i + 5])
        
        edge_index = torch.tensor(edges, dtype=torch.long).t()
        file_paths = [f"src/module_{i}.py" for i in range(num_nodes)]
        
        print(f"Device: {neural_service.device}")
        print(f"Nodes: {num_nodes}")
        print(f"Edges: {edge_index.shape[1]}")
        
        # Measure embedding generation time
        start_time = time.perf_counter()
        
        embeddings = neural_service.train_embeddings(
            edge_index=edge_index,
            num_nodes=num_nodes
        )
        
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        
        # Verify embeddings
        assert embeddings.shape == (num_nodes, 64)
        assert embeddings.device.type == "cpu"
        
        # Report results
        print(f"Time: {elapsed_time:.2f}s ({elapsed_time / 60:.2f} minutes)")
        print(f"Target: < 300s (5 minutes)")
        print(f"Files per second: {num_nodes / elapsed_time:.2f}")
        
        # Verify requirement (5 minutes = 300 seconds)
        assert elapsed_time < 300.0, (
            f"Embedding generation for 100 files took {elapsed_time:.2f}s, "
            f"requirement is < 300s (5 minutes)"
        )
    
    def test_embedding_generation_efficiency(self, neural_service):
        """
        Performance Test: Embedding generation should be efficient.
        
        Validates: Requirements 10.2
        """
        print("\n=== Embedding Generation Efficiency Test ===")
        
        # Test with different graph sizes
        sizes = [25, 50, 100]
        times = []
        
        for size in sizes:
            # Create graph
            edges = [[i, (i + 1) % size] for i in range(size)]
            edge_index = torch.tensor(edges, dtype=torch.long).t()
            
            # Measure time
            start_time = time.perf_counter()
            
            embeddings = neural_service.train_embeddings(
                edge_index=edge_index,
                num_nodes=size
            )
            
            end_time = time.perf_counter()
            elapsed_time = end_time - start_time
            times.append(elapsed_time)
            
            print(f"Size {size}: {elapsed_time:.2f}s ({size / elapsed_time:.2f} files/s)")
        
        # Verify efficiency improves with size (amortized cost decreases)
        time_per_file_25 = times[0] / sizes[0]
        time_per_file_100 = times[2] / sizes[2]
        
        print(f"\nTime per file (25): {time_per_file_25:.3f}s")
        print(f"Time per file (100): {time_per_file_100:.3f}s")
        print(f"Efficiency gain: {time_per_file_25 / time_per_file_100:.2f}x")
        
        # Larger graphs should be more efficient per file
        assert time_per_file_100 <= time_per_file_25 * 1.5, (
            f"Efficiency should improve with size, but 100-file is {time_per_file_100 / time_per_file_25:.2f}x slower per file"
        )


# ============================================================================
# Performance Test 3: Large Repository Handling
# ============================================================================


@pytest.mark.performance
@pytest.mark.slow
class TestLargeRepositoryHandling:
    """Test large repository handling meets requirements."""
    
    def test_large_repository_1000_files(self, neural_service):
        """
        Performance Test: Handle repository with 1000 files.
        
        Validates: Requirements 10.5
        """
        print("\n=== Large Repository Handling Test (1000 files) ===")
        
        # Create dependency graph with 1000 nodes
        num_nodes = 1000
        edges = []
        
        # Create hierarchical structure
        num_modules = 10
        files_per_module = num_nodes // num_modules
        
        for module_idx in range(num_modules):
            module_start = module_idx * files_per_module
            module_end = (module_idx + 1) * files_per_module
            
            # Chain within module
            for i in range(module_start, module_end - 1):
                edges.append([i, i + 1])
            
            # Cross-module dependencies
            if module_idx + 1 < num_modules:
                next_module_start = (module_idx + 1) * files_per_module
                edges.append([module_start, next_module_start])
        
        edge_index = torch.tensor(edges, dtype=torch.long).t()
        
        print(f"Device: {neural_service.device}")
        print(f"Nodes: {num_nodes}")
        print(f"Edges: {edge_index.shape[1]}")
        
        # Measure processing time
        start_time = time.perf_counter()
        
        try:
            embeddings = neural_service.train_embeddings(
                edge_index=edge_index,
                num_nodes=num_nodes
            )
            
            end_time = time.perf_counter()
            elapsed_time = end_time - start_time
            
            # Verify embeddings
            assert embeddings.shape == (num_nodes, 64)
            assert embeddings.device.type == "cpu"
            
            # Report results
            print(f"Time: {elapsed_time:.2f}s ({elapsed_time / 60:.2f} minutes)")
            print(f"Files per second: {num_nodes / elapsed_time:.2f}")
            print(f"Status: SUCCESS")
            
            # Verify reasonable performance
            assert elapsed_time < 1800, (
                f"Processing 1000 files took {elapsed_time:.2f}s (> 30 minutes), "
                f"this is too slow"
            )
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"Status: OUT OF MEMORY")
                pytest.skip("GPU out of memory - expected on limited hardware")
            else:
                raise
    
    def test_large_repository_memory_usage(self, neural_service):
        """
        Performance Test: Verify memory usage is reasonable for large repositories.
        
        Validates: Requirements 10.5
        """
        print("\n=== Large Repository Memory Usage Test ===")
        
        # Test with 500 nodes
        num_nodes = 500
        edges = [[i, (i + 1) % num_nodes] for i in range(num_nodes)]
        edge_index = torch.tensor(edges, dtype=torch.long).t()
        
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            initial_memory = torch.cuda.memory_allocated() / 1e9
            print(f"Initial GPU memory: {initial_memory:.2f} GB")
        
        # Train embeddings
        embeddings = neural_service.train_embeddings(
            edge_index=edge_index,
            num_nodes=num_nodes
        )
        
        if torch.cuda.is_available():
            peak_memory = torch.cuda.max_memory_allocated() / 1e9
            final_memory = torch.cuda.memory_allocated() / 1e9
            
            print(f"Peak GPU memory: {peak_memory:.2f} GB")
            print(f"Final GPU memory: {final_memory:.2f} GB")
            print(f"Memory per node: {(peak_memory - initial_memory) / num_nodes * 1000:.2f} MB")
            
            # Verify memory usage is reasonable (< 10 MB per node)
            memory_per_node_mb = (peak_memory - initial_memory) / num_nodes * 1000
            assert memory_per_node_mb < 10.0, (
                f"Memory usage is {memory_per_node_mb:.2f} MB per node, should be < 10 MB"
            )
        else:
            print("CPU mode - no memory tracking")
        
        # Clean up
        del embeddings
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


# ============================================================================
# Performance Test 4: End-to-End Workflow
# ============================================================================


@pytest.mark.performance
class TestEndToEndWorkflow:
    """Test end-to-end workflow performance."""
    
    def test_complete_workflow_performance(self, cloud_api_client, neural_service, mock_redis):
        """
        Performance Test: Complete workflow from API to embeddings.
        
        Validates: Requirements 10.1, 10.2
        """
        print("\n=== End-to-End Workflow Performance Test ===")
        
        # Step 1: API dispatch
        api_start = time.perf_counter()
        
        response = cloud_api_client.post(
            "/api/v1/ingestion/ingest/github.com/e2e/test-repo",
            headers={"Authorization": "Bearer test-token-123"}
        )
        
        api_end = time.perf_counter()
        api_time = (api_end - api_start) * 1000
        
        assert response.status_code == 200
        print(f"API dispatch: {api_time:.2f}ms")
        
        # Step 2: Embedding generation (simulate with 50 files)
        num_nodes = 50
        edges = [[i, (i + 1) % num_nodes] for i in range(num_nodes)]
        edge_index = torch.tensor(edges, dtype=torch.long).t()
        
        embedding_start = time.perf_counter()
        
        embeddings = neural_service.train_embeddings(
            edge_index=edge_index,
            num_nodes=num_nodes
        )
        
        embedding_end = time.perf_counter()
        embedding_time = embedding_end - embedding_start
        
        print(f"Embedding generation: {embedding_time:.2f}s")
        
        # Step 3: Total workflow time
        total_time = api_time / 1000 + embedding_time
        
        print(f"\nTotal workflow time: {total_time:.2f}s")
        print(f"API overhead: {(api_time / 1000 / total_time) * 100:.1f}%")
        print(f"Embedding time: {(embedding_time / total_time) * 100:.1f}%")
        
        # Verify API overhead is minimal (< 5% of total time)
        api_overhead_pct = (api_time / 1000 / total_time) * 100
        assert api_overhead_pct < 5.0, (
            f"API overhead is {api_overhead_pct:.1f}%, should be < 5%"
        )


# ============================================================================
# Summary Report
# ============================================================================


@pytest.mark.performance
def test_generate_performance_summary():
    """
    Generate a comprehensive performance test summary.
    
    This test always passes and just generates a summary report.
    """
    print("\n" + "=" * 70)
    print("PHASE 19 PERFORMANCE TEST SUMMARY")
    print("=" * 70)
    print("\nPerformance Requirements:")
    print("  1. API dispatch latency: < 100ms (P95)")
    print("  2. Embedding generation: < 5 minutes for 100 files")
    print("  3. Large repository: Handle 1000+ files")
    print("\nTest Coverage:")
    print("  - API latency and consistency")
    print("  - Embedding generation time and efficiency")
    print("  - Large repository handling and memory usage")
    print("  - End-to-end workflow performance")
    print("\nRun tests with: pytest tests/performance/test_phase19_performance.py -v")
    print("=" * 70)
